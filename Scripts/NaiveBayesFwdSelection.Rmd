---
title: "Naive Bayes Forward Subset Selection"
output: html_document
---

## Load Data and Necessary Libraries

```{r, load-libs}
# For naive Bayes function
library(e1071)
```

```{r, load-data}
# Load validation and training sets
df.val <- read.csv("Data/set_splits/val_codons.csv")
df.train <- read.csv("Data/set_splits/train_codons.csv")
# Encode Kingdom as a factor
df.val$Kingdom <- as.factor(df.val$Kingdom)
df.train$Kingdom <- as.factor(df.train$Kingdom)
```

## Naive Bayes Forward Subset Selection Functions

Although cross-validation would give better performance estimates, it becomes quite computationally expensive when doing multinomial classification using 64 predictors. In a forward subset selection framework using the typical 10-fold cross-validation approach, identifying the first predictor would require the creation of 6,400 different models. This becomes prohibitively computationally expensive on a local machine, so a single validation set will be used. Because there are a comparatively large amount of observations, the validation set has over 2,500 observations and the training set over 7,750, so there is not the same need for cross-validation as there would be with a smaller data set.

```{r, nb-funcs}
# Function to calculate macro F1 score for each model iteration
macro_f1 <- function(true, pred){
  classes <- levels(true)
  f1 <- numeric(length(classes))
  for (i in 1:length(classes)){
    class <- classes[i]
    TP <- sum(true == class & pred == class)
    FP <- sum(true != class & pred == class)
    FN <- sum(true == class & pred != class)
    # Make sure to not divide by 0
    precision <- ifelse(TP+FP == 0, 0, TP/(TP+FP))
    recall <- ifelse(TP+FN == 0, 0, TP/(TP+FN))
    f1[i] <- ifelse(precision+recall == 0, 0, 2 * precision * recall / (precision + recall))
  }
  return(mean(f1))
}

# Function to do naive Bayes forward subset selection
nb_fwd_selection <- function(train_data, val_data, response, verbose = TRUE){
  # Set up variables to keep track of features
  all_feat <- setdiff(names(train_data), response)
  sel <- character(0)
  rest <- all_feat
  best_score <- 0
  repeat{
    results <- data.frame(feature = rest, f1_score = 0)
    for (feat in rest){
      # Data frame with response, features already selected, and feature being tested
      training_df <- train_data[, c(response, sel, feat)]
      # Fit model then predict using validation data
      fit <- naiveBayes(as.formula(paste(response, "~ .")), training_df)
      pred <- predict(fit, val_data)
      # Calculate and save macro f1 score
      f1 <- macro_f1(val_data[[response]], pred)
      results$f1_score[results$feature == feat] <- f1
    }
    
    # Find feature that improves performance most
    best_i <- which.max(results$f1_score)
    best_feat <- results$feature[best_i]
    best_score_new <- results$f1_score[best_i]
    
    # Determine if new score is an improvement
    if (best_score_new > best_score){
      sel <- c(sel, best_feat)
      rest <- setdiff(rest, best_feat)
      best_score <- best_score_new
      if (verbose) cat("Selected feature:", best_feat, "New macro F1 score =", round(best_score, 4), "\n")
    } else {
      if (verbose) cat("No improvement found.")
      break
    }
  }
  return(list(selected_features = sel, macro_f1 = best_score))
}
```

## Fitting Naive Bayes Model

```{r, nb-var-sel}
# Variable selection
nb_features <- nb_fwd_selection(df.train, df.val, "Kingdom")

# Fit and save best model
## Use training and validation sets for final model to improve performance on test set
df <- rbind(df.train, df.val)
nb_fit <- naiveBayes(as.formula(paste("Kingdom ~", paste(nb_features$selected_features, collapse = "+"))), df)
saveRDS(nb_fit, "Fits/NaiveBayes.rds")
```

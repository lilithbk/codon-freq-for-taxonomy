---
title: "Fitting Neural Networks"
output: html_document
---

## Load Data and Necessary Libraries

```{r, load-libs}
# For neural networks
library(keras)
reticulate::use_condaenv(condaenv = "r-tensorflow")
```

```{r, load-data}
# Load validation and training sets
df.val <- read.csv("Data/set_splits/val_codons.csv")
df.train <- read.csv("Data/set_splits/train_codons.csv")
# Encode Kingdom as a factor
df.val$Kingdom <- as.factor(df.val$Kingdom)
df.train$Kingdom <- as.factor(df.train$Kingdom)
```

## Prepare Data for Fitting

```{r, prep-data}
# Set up data
x.train <- model.matrix(Kingdom ~ . -1, df.train) %>% scale()
y.train <- to_categorical(as.integer(df.train$Kingdom) - 1, 6)
x.val <- model.matrix(Kingdom ~ . -1, df.val) %>% scale()
y.val <- to_categorical(as.integer(df.val$Kingdom) - 1, 6)

# Define function to stop early if there are no improvements and save computation time
early_stop <- callback_early_stopping(patience = 15, restore_best_weights = TRUE)
```

## Single Layer Neural Networks

```{r, single-layer-10-nodes}
# Define first model
nn.1.layer.10.units <- keras_model_sequential() %>%
  layer_dense(units = 10, activation = "relu", input_shape = 64) %>%
  layer_dense(units = 6, activation = "softmax")
# Compile network
nn.1.layer.10.units %>% compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
# Fit network
nn.1.layer.10.units %>% fit(x.train, y.train, epochs = 200, batch_size = 64, validation_data = list(x.val, y.val), callbacks = list(early_stop))
# val_loss: 0.2759 - val_accuracy: 0.9152
```

```{r, single-layer-20-nodes}
# Define first model
nn.1.layer.20.units <- keras_model_sequential() %>%
  layer_dense(units = 20, activation = "relu", input_shape = 64) %>%
  layer_dense(units = 6, activation = "softmax")
# Compile network
nn.1.layer.20.units %>% compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
# Fit network
nn.1.layer.20.units %>% fit(x.train, y.train, epochs = 200, batch_size = 64, validation_data = list(x.val, y.val), callbacks = list(early_stop))
# val_loss: 0.2251 - val_accuracy: 0.9383
```

```{r, single-layer-30-nodes}
# Define first model
nn.1.layer.30.units <- keras_model_sequential() %>%
  layer_dense(units = 30, activation = "relu", input_shape = 64) %>%
  layer_dense(units = 6, activation = "softmax")
# Compile network
nn.1.layer.30.units %>% compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
# Fit network
nn.1.layer.30.units %>% fit(x.train, y.train, epochs = 200, batch_size = 64, validation_data = list(x.val, y.val), callbacks = list(early_stop))
# val_loss: 0.2103 - val_accuracy: 0.9375
```

```{r, single-layer-40-nodes}
# Define first model
nn.1.layer.40.units <- keras_model_sequential() %>%
  layer_dense(units = 40, activation = "relu", input_shape = 64) %>%
  layer_dense(units = 6, activation = "softmax")
# Compile network
nn.1.layer.40.units %>% compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
# Fit network
nn.1.layer.40.units %>% fit(x.train, y.train, epochs = 200, batch_size = 64, validation_data = list(x.val, y.val), callbacks = list(early_stop))
# val_loss: 0.2024 - val_accuracy: 0.9418
```

```{r, single-layer-50-nodes}
# Define first model
nn.1.layer.50.units <- keras_model_sequential() %>%
  layer_dense(units = 50, activation = "relu", input_shape = 64) %>%
  layer_dense(units = 6, activation = "softmax")
# Compile network
nn.1.layer.50.units %>% compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
# Fit network
nn.1.layer.50.units %>% fit(x.train, y.train, epochs = 200, batch_size = 64, validation_data = list(x.val, y.val), callbacks = list(early_stop))
# val_loss: 0.1822 - val_accuracy: 0.9514
```

## Single-Layer Neural Networks with Dropout

```{r, single-layer-10-dropout-0.2}
# Define model
nn.1.layer.10.dropout.2 <- keras_model_sequential() %>%
  layer_dense(units = 10, activation = "relu", input_shape = 64) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 6, activation = "softmax")
# Compile network
nn.1.layer.10.dropout.2 %>% compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
# Fit network
nn.1.layer.10.dropout.2 %>% fit(x.train, y.train, epochs = 200, batch_size = 64, validation_data = list(x.val, y.val), callbacks = list(early_stop))
# val_loss: 0.3352 - val_accuracy: 0.8966
```

```{r, single-layer-10-dropout-0.1}
# Define model
nn.1.layer.10.dropout.1 <- keras_model_sequential() %>%
  layer_dense(units = 10, activation = "relu", input_shape = 64) %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 6, activation = "softmax")
# Compile network
nn.1.layer.10.dropout.1 %>% compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
# Fit network
nn.1.layer.10.dropout.1 %>% fit(x.train, y.train, epochs = 200, batch_size = 64, validation_data = list(x.val, y.val), callbacks = list(early_stop))
# val_loss: 0.3239 - val_accuracy: 0.8970
```

Overfitting does not seem to be a major concern with this data in a single-layer NN because adding a dropout layer worsens accuracy and loss.

## Two-Layer Neural Networks

```{r, two-layer-40-10}
# Define model
nn.2.layers.40.10 <- keras_model_sequential() %>%
  layer_dense(units = 40, activation = "relu", input_shape = 64) %>%
  layer_dense(units = 10, activation = "relu", input_shape = 40) %>%
  layer_dense(units = 6, activation = "softmax")
# Compile network
nn.2.layers.40.10 %>% compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
# Fit network
nn.2.layers.40.10 %>% fit(x.train, y.train, epochs = 200, batch_size = 64, validation_data = list(x.val, y.val), callbacks = list(early_stop))
# val_loss: 0.2334 - val_accuracy: 0.9429
```

```{r, two-layer-40-20}
# Define model
nn.2.layers.40.20 <- keras_model_sequential() %>%
  layer_dense(units = 40, activation = "relu", input_shape = 64) %>%
  layer_dense(units = 20, activation = "relu", input_shape = 40) %>%
  layer_dense(units = 6, activation = "softmax")
# Compile network
nn.2.layers.40.20 %>% compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
# Fit network
nn.2.layers.40.20 %>% fit(x.train, y.train, epochs = 200, batch_size = 64, validation_data = list(x.val, y.val), callbacks = list(early_stop))
# val_loss: 0.2272 - val_accuracy: 0.9395
```

```{r, two-layer-50-10}
# Define model
nn.2.layers.50.10 <- keras_model_sequential() %>%
  layer_dense(units = 50, activation = "relu", input_shape = 64) %>%
  layer_dense(units = 10, activation = "relu", input_shape = 50) %>%
  layer_dense(units = 6, activation = "softmax")
# Compile network
nn.2.layers.50.10 %>% compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
# Fit network
nn.2.layers.50.10 %>% fit(x.train, y.train, epochs = 200, batch_size = 64, validation_data = list(x.val, y.val), callbacks = list(early_stop))
# val_loss: 0.2053 - val_accuracy: 0.9460
```

```{r, two-layer-50-20}
# Define model
nn.2.layers.50.20 <- keras_model_sequential() %>%
  layer_dense(units = 50, activation = "relu", input_shape = 64) %>%
  layer_dense(units = 20, activation = "relu", input_shape = 50) %>%
  layer_dense(units = 6, activation = "softmax")
# Compile network
nn.2.layers.50.20 %>% compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
# Fit network
nn.2.layers.50.20 %>% fit(x.train, y.train, epochs = 200, batch_size = 64, validation_data = list(x.val, y.val), callbacks = list(early_stop))
# val_loss: 0.2369 - val_accuracy: 0.9406
```

## Fitting Final Neural Network

```{r, fit-NN}
df <- rbind(df.train, df.val)
# The best performing network for validation loss and accuracy had a single hidden layer with 50 units
nn.final <- keras_model_sequential() %>%
  layer_dense(units = 50, activation = "relu", input_shape = 64) %>%
  layer_dense(units = 6, activation = "softmax")
# Compile
nn.final %>% compile(loss = "categorical_crossentropy", optimizer = "adam", metrics = "accuracy")
# Fit
nn.final %>% fit(x.train, y.train, epochs = 75, batch_size = 64)
save_model_tf(nn.final, "Fits/NeuralNetwork")
```

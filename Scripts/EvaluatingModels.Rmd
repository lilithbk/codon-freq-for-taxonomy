---
title: "Evaluating Models"
output: html_document
---

## Load Libraries, Data, and Models

```{r, load-libs}
# For the models
library(e1071)
library(randomForest)
library(keras)
reticulate::use_condaenv(condaenv = "r-tensorflow")
# For confusion matrices
library(caret)
# For plotting
library(tidyverse)
# For ROC curves
library(multiROC)
# For converting data frames into latex tables
library(xtable)
```

```{r, load-data}
# Load testing set
df.test <- read.csv("Data/set_splits/test_codons.csv")
# Encode Kingdom as a factor
df.test$Kingdom <- as.factor(df.test$Kingdom)
# Some additional data manipulation for compatibility with keras
y.test <- df.test$Kingdom
x.test <- model.matrix(Kingdom ~ . - 1, df.test) %>% scale()
```

```{r, load-models}
# Load naive Bayes model
fit.nb <- readRDS("Fits/NaiveBayes.rds")
# Load random forest
fit.rf <- readRDS("Fits/RandomForest.rds")
# Load neural network
fit.nn <- load_model_tf("Fits/NeuralNetwork")
```

## Use Test Set to Predict Classes and Probabilities

```{r}
# Naive Bayes
class.nb <- predict(fit.nb, df.test)
prob.nb <- predict(fit.nb, df.test, type = "raw")

# Random forest
class.rf <- predict(fit.rf, df.test)
prob.rf <- predict(fit.rf, df.test, type = "prob")
## The random forest prediction function retains the original indices, so removing them now
rownames(prob.rf) <- NULL

# Neural network
prob.nn <- predict(fit.nn, x.test)
colnames(prob.nn) <- levels(df.test$Kingdom) #Rename columns
## Define function to reverse categorical encoding
int_to_char <- function(out){
  out[out == 0] <- "arc"
  out[out == 1] <- "bct"
  out[out == 2] <- "inv"
  out[out == 3] <- "pln"
  out[out == 4] <- "vrl"
  out[out == 5] <- "vrt"
  return(out)
}
## Extract class with highest probability for each test observation and convert back to their original labels
class.nn <- k_argmax(prob.nn) %>% as.numeric() %>% int_to_char() %>% as.factor()
```

## Generate and Plot Confusion Matrices

```{r, make-conf-mtx}
# Naive Bayes
cm.nb <- confusionMatrix(class.nb, df.test$Kingdom)
# Random forest
cm.rf <- confusionMatrix(class.rf, df.test$Kingdom, mode = "everything")
# Neural network
cm.nn <- confusionMatrix(class.nn, df.test$Kingdom, mode = "everything")
```

```{r, conf-mtx-plotting-func}
# The function in this chunk was adapted using code from user davedgd on stack overflow
# Link to thread: https://stackoverflow.com/questions/37897252/plot-confusion-matrix-in-r-using-ggplot/59119854#59119854

plot_conf_matrix <- function(conf_mtx, plot_name = NULL){
  # cm: confusion matrix generated by caret library
  # Prepare table for plotting
  cm <- data.frame(conf_mtx$table) %>%
    mutate(Correct = ifelse(Prediction == Reference, "Yes", "No")) %>%
    group_by(Reference) %>%
    mutate(Proportion = Freq/sum(Freq))
  # Generate plot
  plot.cm <- ggplot(cm, aes(Reference, Prediction, fill = Correct, alpha = Proportion)) +
    geom_tile() +
    geom_text(aes(label = Freq), vjust = 0.5, fontface = "bold", alpha = 1) +
    scale_fill_manual(values = c(Yes = "limegreen", No = "red")) +
    theme_bw()
  if (!(is.null(plot_name))){
    ggsave(plot_name, plot.cm)
  } else {
    print(plot.cm)
  }
}
```

```{r, plot-conf-mtx}
# Naive Bayes
plot_conf_matrix(cm.nb, "Plots/NaiveBayesConfMatrix.png")
# Random forest
plot_conf_matrix(cm.rf, "Plots/RandomForestConfMatrix.png")
# Neural network
plot_conf_matrix(cm.nn, "Plots/NeuralNetworkConfMatrix.png")
```

## Plot ROC Curves

```{r, ROC-curves}
# Creating ROC curves for multinomial classification is complicated, so multiROC has a very specific format that the input data frame must be in

# First, putting the true classes into the correct format
true_cols <- lapply(levels(y.test), function(level) ifelse(y.test == level, 1, 0))
names(true_cols) <- paste0(levels(y.test), "_true")
# Formatting predicted probabilities from naive Bayes model
nb_cols <- lapply(levels(y.test), function(level) prob.nb[, level])
names(nb_cols) <- paste0(levels(y.test), "_pred_NB")
# Repeat for random forest
rf_cols <- lapply(levels(y.test), function(level) prob.rf[, level])
names(rf_cols) <- paste0(levels(y.test), "_pred_RF")
# And again for neural network
nn_cols <- lapply(levels(y.test), function(level) prob.nn[, level])
names(nn_cols) <- paste0(levels(y.test), "_pred_NN")

# Consolidate all lists into a data frame
all_cols <- as.data.frame(c(true_cols, nb_cols, rf_cols, nn_cols))
# The multi_roc function causes many warnings because there are many ties among the probabilities, but with multiple classes this is to be expected, so these warnings are not particularly concerning
roc_df <- multi_roc(all_cols)
# Create a ggplot-friendly data frame
plot_roc_df <- plot_roc_data(roc_df)
```

```{r, plot-ROC-curves}
# Create and save ROC curve for naive Bayes model
plot_roc_df %>%
  filter(!(Group %in% c("Micro", "Macro"))) %>%
  filter(Method == "NB") %>%
  ggplot(aes(1-Specificity, Sensitivity, color = Group)) +
  geom_line(linewidth = 1, alpha = 0.7) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", linewidth = 1, color = "gray") +
  ylab("True Positive Rate") +
  xlab("False Positive Rate") +
  theme_bw()
ggsave("Plots/NaiveBayesROCCurve.png")

# Repeat for random forest
plot_roc_df %>%
  filter(!(Group %in% c("Micro", "Macro"))) %>%
  filter(Method == "RF") %>%
  ggplot(aes(1-Specificity, Sensitivity, color = Group)) +
  geom_line(linewidth = 1, alpha = 0.7) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", linewidth = 1, color = "gray") +
  ylab("True Positive Rate") +
  xlab("False Positive Rate") +
  theme_bw()
ggsave("Plots/RandomForestROCCurve.png")

# And again for neural network
plot_roc_df %>%
  filter(!(Group %in% c("Micro", "Macro"))) %>%
  filter(Method == "NN") %>%
  ggplot(aes(1-Specificity, Sensitivity, color = Group)) +
  geom_line(linewidth = 1, alpha = 0.7) +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", linewidth = 1, color = "gray") +
  ylab("True Positive Rate") +
  xlab("False Positive Rate") +
  theme_bw()
ggsave("Plots/NeuralNetworkROCCurve.png")
```

## Extracting Evaluation Metrics

```{r, macro-metrics}
# Create list of confusion matrices to iterate over
matrices <- list(cm.nb, cm.rf, cm.nn)
# Initialize vectors to store macro performance metrics
acc <- c()
prec <- c()
rec <- c()
f1 <- c()
# Iterate over matrices
for (i in matrices){
  # Accuracy
  acc <- c(acc, round(i$overall["Accuracy"], 3))
  # Macro-averaged precision
  prec <- c(prec, round(mean(i$byClass[, "Precision"]), 3))
  # Macro-averaged recall
  rec <- c(rec, round(mean(i$byClass[, "Recall"]), 3))
  # Macro F1 score
  f1 <- c(f1, round(mean(i$byClass[, "F1"]), 3))
}
# Initialize vector for macro AUC
auc <- c()
# Iterate over each method and pull macro AUC from data frame from multiROC
methods <- list("NB", "RF", "NN")
for (method in methods){
  auc <- c(auc, round(plot_roc_df %>%
    filter(Group == "Macro") %>%
    filter(Method == method) %>%
    distinct(AUC) %>%
    pull(), 3))
}
# Combine the metrics into a single data frame
overall_df <- tibble(
  Model = c("Naive Bayes", "Random Forest", "Neural Network"),
  Accuracy = acc,
  Precision = prec,
  Recall = rec,
  "Macro F1 Score" = f1,
  "Macro AUC" = auc
)

xtable(overall_df, digits = 3)
```

```{r, per-class-metrics}
# Function to extract per-class performance metrics
per_class_metrics <- function(true, pred, cm, roc_df, method){
  # Per-class accuracy and AUC
  classes <- levels(true)
  total <- length(true)
  class_acc <- c()
  class_auc <- c()
  for(i in 1:length(classes)){
    class <- classes[i]
    TP <- sum(true == class & pred == class)
    TN <- sum(true != class & pred != class)
    class_acc <- c(class_acc, round((TP+TN)/total, 3))
    # AUC  
    class_auc <- c(class_auc, round(roc_df %>%
      filter(Method == method) %>%
      filter(Group == class) %>%
      distinct(AUC) %>%
      pull(), 3))
  }
  # Per-class precision
  class_prec <- round(unname(cm$byClass[, "Precision"]), 3)
  # Per-class recall
  class_rec <- round(unname(cm$byClass[, "Recall"]), 3)
  # F1 scores
  f1 <- round(unname(cm$byClass[, "F1"]), 3)
  return(list(Accuracy = class_acc, Precision = class_prec, Recall = class_rec, "F1 Score" = f1, AUC = class_auc))
}

# Make table for naive Bayes
nb_df <- as_tibble(per_class_metrics(y.test, class.nb, cm.nb, plot_roc_df, "NB"))
xtable(nb_df, digits = 3)

# Make table for random forest
rf_df <- as_tibble(per_class_metrics(y.test, class.rf, cm.rf, plot_roc_df, "RF"))
xtable(rf_df, digits = 3)

# Make table for neural network
nn_df <- as_tibble(per_class_metrics(y.test, class.nn, cm.nn, plot_roc_df, "NN"))
xtable(nn_df, digits = 3)
```

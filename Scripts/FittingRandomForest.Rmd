---
title: "Fitting Random Forest"
output: html_document
---

## Load Data and Necessary Libraries

```{r, load-libs}
# For random forest function
library(randomForest)
```

```{r, load-data}
# Load validation and training sets
df.val <- read.csv("Data/set_splits/val_codons.csv")
df.train <- read.csv("Data/set_splits/train_codons.csv")
# Encode Kingdom as a factor
df.val$Kingdom <- as.factor(df.val$Kingdom)
df.train$Kingdom <- as.factor(df.train$Kingdom)
```

## Tune Random Forest Hyperparameters

```{r, define-RF-func}
# Function to calculate macro F1 score for each model iteration
macro_f1 <- function(true, pred){
  classes <- levels(true)
  f1 <- numeric(length(classes))
  for (i in 1:length(classes)){
    class <- classes[i]
    TP <- sum(true == class & pred == class)
    FP <- sum(true != class & pred == class)
    FN <- sum(true == class & pred != class)
    # Make sure to not divide by 0
    precision <- ifelse(TP+FP == 0, 0, TP/(TP+FP))
    recall <- ifelse(TP+FN == 0, 0, TP/(TP+FN))
    f1[i] <- ifelse(precision+recall == 0, 0, 2 * precision * recall / (precision + recall))
  }
  return(mean(f1))
}

# Function to tune mtry and ntree for random forest
rf_tune_hyperparams <- function(train_data, val_data, response, mtry, ntree, verbose = TRUE, seed = 27){
  # Set seed to ensure reproducible results
  set.seed(seed)
  
  # Initialize matrix to store macro F1 scores
  results <- matrix(nrow = length(mtry), ncol = length(ntree))
  
  # Iterate through all mtry and ntree values
  for (i in 1:length(mtry)){
    for (j in 1:length(ntree)){
      fit <- randomForest(as.formula(paste(response, "~ .")), train_data, mtry = mtry[i], ntree = ntree[j])
      pred <- predict(fit, val_data)
      results[i, j] <- macro_f1(val_data[[response]], pred)
      if (verbose) cat("mtry = ", mtry[i], "and ntree = ", ntree[j], "resulted in macro F1 score of ", results[i, j], "\n")
    }
  }
  # Extract best values for ntree and mtry
  best_score <- max(results)
  best_i <- which(results == best_score, arr.ind = TRUE)
  best_mtry <- mtry[best_i[, 1]]
  best_ntree <- ntree[best_i[, 2]]
  rownames(results) <- mtry
  colnames(results) <- ntree  
  list(best_mtry = best_mtry, best_ntree = best_ntree, best_macro_f1 = best_score, matrix = results)
}
```

```{r, tune-RF}
rf_tuning <- rf_tune_hyperparams(df.train, df.val, "Kingdom", seq(6, 12), seq(250, 1000, 250))
rf_tuning
```

## Fit Final Random Forest Model

```{r, fit-RF}
# Set seed for reproducibility
set.seed(27)
# Combine training and validation sets for final model
df <- rbind(df.train, df.val)
# Use mtry and ntree with best macro F1 score
rf_fit <- randomForest(Kingdom ~ ., df, mtry = 12, ntree = 12, importance = TRUE)
# Save for later evaluation
saveRDS(rf_fit, "Fits/RandomForest.rds")
```
